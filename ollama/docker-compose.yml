services:
  ollama:
    image: ollama/ollama:0.11.10@sha256:a5409cb903d30f9cd67e9f430dd336ddc9274e16fd78f75b675c42065991b4fd
    ports:
      - 11434:11434
    labels:
      - traefik.enable=true
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./code:/code
      - /mnt/ssd512/ctr_data/ollama/ollama:/root/.ollama
    container_name: ollama
    tty: true
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  openwebui:
    image: ghcr.io/open-webui/open-webui:main@sha256:6411dd62178b3d5fbd26bd953b12c03e74ede96875aa96c394513628ba842cc1
    container_name: openwebui
    labels:
      - traefik.enable=true
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./openwebui:/app/backend/data
    depends_on:
      - ollama
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://ollama:11434 #comma separated ollama hosts
      - ENV=prod
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=REMOTE-EMAIL
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=REMOTE-NAME
    healthcheck:
      test: "curl -f http://localhost:8080/health"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
  deepresearch:
    image: localdeepresearch/local-deep-research:1.2.0@sha256:40f6bd47585e9160b3392229a05b2837462f36f8ebd7c17a9c4f74050ba73e91
    restart: unless-stopped
    container_name: deepresearch
    labels:
      - traefik.enable=true
      - traefik.http.services.deepresearch.loadbalancer.server.port=5000
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # LLM Configuration
      - LDR_LLM__PROVIDER=ollama
      - LDR_LLM__MODEL=gemma3
      - LDR_LLM__TEMPERATURE=0.7
      
      # Search Configuration
      - LDR_SEARCH__TOOL=auto
      - LDR_SEARCH__ITERATIONS=2
      - LDR_SEARCH__QUESTIONS_PER_ITERATION=2

      # Searxng Configuration
      - SEARXNG_INSTANCE=http://searxng:8080
      - SEARXNG_DELAY=2.0
      
      # Web Interface Settings
      - LDR_WEB__PORT=5000
      - LDR_WEB__HOST=0.0.0.0
    volumes:
      - ./deepresearch:/root/.config/local_deep_research
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest@sha256:9c071b8685d6586692072df2eaa48a0adb8e53d13e18cb53e21924837c5803d7
    restart: unless-stopped
    labels:
      - traefik.enable=true
      - traefik.http.services.searxng.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    healthcheck:
      test: "wget --no-verbose --tries=1 --spider http://localhost:8080/ || exit 1"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
    # cap_drop:
    #   - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
networks:
  default:
    external: true
    name: ctr-network