services:
  ollama:
    image: ollama/ollama:0.9.5@sha256:64fcc2a7c48ae920f5317264031d86414e30417269631822858c6d23f61100b0
    ports:
      - 11434:11434
    labels:
      - traefik.enable=true
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./code:/code
      - /mnt/ssd512/ctr_data/ollama/ollama:/root/.ollama
    container_name: ollama
    tty: true
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  openwebui:
    image: ghcr.io/open-webui/open-webui:main@sha256:a5ce467e33749013c2fa6cba671499be92c1a8c3a475b8e3dc0eff554427aaf8
    container_name: openwebui
    labels:
      - traefik.enable=true
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./openwebui:/app/backend/data
    depends_on:
      - ollama
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://ollama:11434 #comma separated ollama hosts
      - ENV=prod
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=REMOTE-EMAIL
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=REMOTE-NAME
    healthcheck:
      test: "curl -f http://localhost:8080/health"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
  deepresearch:
    image: localdeepresearch/local-deep-research:0.6.4@sha256:75d12092a3570089ec5629857baab70d6c77dc440ffc14f929af41606bc142d3
    restart: unless-stopped
    container_name: deepresearch
    labels:
      - traefik.enable=true
      - traefik.http.services.deepresearch.loadbalancer.server.port=5000
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # LLM Configuration
      - LDR_LLM__PROVIDER=ollama
      - LDR_LLM__MODEL=gemma3
      - LDR_LLM__TEMPERATURE=0.7
      
      # Search Configuration
      - LDR_SEARCH__TOOL=auto
      - LDR_SEARCH__ITERATIONS=2
      - LDR_SEARCH__QUESTIONS_PER_ITERATION=2

      # Searxng Configuration
      - SEARXNG_INSTANCE=http://searxng:8080
      - SEARXNG_DELAY=2.0
      
      # Web Interface Settings
      - LDR_WEB__PORT=5000
      - LDR_WEB__HOST=0.0.0.0
    volumes:
      - ./deepresearch:/root/.config/local_deep_research
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest@sha256:8f757f615fe5dc634ba16fae8576a5189b695b55fc225f6bae2efb5b9aa8d3da
    restart: unless-stopped
    labels:
      - traefik.enable=true
      - traefik.http.services.searxng.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    healthcheck:
      test: "wget --no-verbose --tries=1 --spider http://localhost:8080/ || exit 1"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
    # cap_drop:
    #   - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
networks:
  default:
    external: true
    name: ctr-network