services:
  ollama:
    image: ollama/ollama:0.6.8@sha256:50ab2378567a62b811a2967759dd91f254864c3495cbe50576bd8a85bc6edd56
    ports:
      - 11434:11434
    labels:
      - traefik.enable=true
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./code:/code
      - /mnt/ssd512/ctr_data/ollama/ollama:/root/.ollama
    container_name: ollama
    tty: true
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  openwebui:
    image: ghcr.io/open-webui/open-webui:main@sha256:49dda8e3c54875074a5edfaac61a2e5e19fa4112f023a783790c86a1cb85b51c
    container_name: openwebui
    labels:
      - traefik.enable=true
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./openwebui:/app/backend/data
    depends_on:
      - ollama
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://ollama:11434 #comma separated ollama hosts
      - ENV=prod
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=REMOTE-EMAIL
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=REMOTE-NAME
    healthcheck:
      test: "curl -f http://localhost:8080/health"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
  deepresearch:
    image: localdeepresearch/local-deep-research:0.3.3@sha256:c111da6c83b0070d4a6be9c436198dc2e852ce3bf24875f6b16743de5fc110c8
    restart: unless-stopped
    container_name: deepresearch
    labels:
      - traefik.enable=true
      - traefik.http.services.deepresearch.loadbalancer.server.port=5000
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # LLM Configuration
      - LDR_LLM__PROVIDER=ollama
      - LDR_LLM__MODEL=gemma3
      - LDR_LLM__TEMPERATURE=0.7
      
      # Search Configuration
      - LDR_SEARCH__TOOL=auto
      - LDR_SEARCH__ITERATIONS=2
      - LDR_SEARCH__QUESTIONS_PER_ITERATION=2

      # Searxng Configuration
      - SEARXNG_INSTANCE=http://searxng:8080
      - SEARXNG_DELAY=2.0
      
      # Web Interface Settings
      - LDR_WEB__PORT=5000
      - LDR_WEB__HOST=0.0.0.0
    volumes:
      - ./deepresearch:/root/.config/local_deep_research
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest@sha256:9636c9ddae4f4d3cd624b7f1d1cfcfc816e4f98e8deb7d8ef475067166fe9af3
    restart: unless-stopped
    labels:
      - traefik.enable=true
      - traefik.http.services.searxng.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    healthcheck:
      test: "wget --no-verbose --tries=1 --spider http://localhost:8080/ || exit 1"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
    # cap_drop:
    #   - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
networks:
  default:
    external: true
    name: ctr-network