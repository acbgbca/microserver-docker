services:
  ollama:
    image: ollama/ollama:0.7.1@sha256:de659b95818c5ea17dc287a2e4f147f81e202d84f1dc4ad3f256c53fb81e8dd0
    ports:
      - 11434:11434
    labels:
      - traefik.enable=true
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./code:/code
      - /mnt/ssd512/ctr_data/ollama/ollama:/root/.ollama
    container_name: ollama
    tty: true
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  openwebui:
    image: ghcr.io/open-webui/open-webui:main@sha256:a73dcd1a6d95b6f295cd991588c5df013a0c26a05b1d555aaa94b1d1cf560b39
    container_name: openwebui
    labels:
      - traefik.enable=true
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./openwebui:/app/backend/data
    depends_on:
      - ollama
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://ollama:11434 #comma separated ollama hosts
      - ENV=prod
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=REMOTE-EMAIL
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=REMOTE-NAME
    healthcheck:
      test: "curl -f http://localhost:8080/health"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
  deepresearch:
    image: localdeepresearch/local-deep-research:0.4.2@sha256:0028d0e38f80d8b41eb73575f708bcf9c4d5c5c5c8b71350d64ae8b8aee50275
    restart: unless-stopped
    container_name: deepresearch
    labels:
      - traefik.enable=true
      - traefik.http.services.deepresearch.loadbalancer.server.port=5000
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # LLM Configuration
      - LDR_LLM__PROVIDER=ollama
      - LDR_LLM__MODEL=gemma3
      - LDR_LLM__TEMPERATURE=0.7
      
      # Search Configuration
      - LDR_SEARCH__TOOL=auto
      - LDR_SEARCH__ITERATIONS=2
      - LDR_SEARCH__QUESTIONS_PER_ITERATION=2

      # Searxng Configuration
      - SEARXNG_INSTANCE=http://searxng:8080
      - SEARXNG_DELAY=2.0
      
      # Web Interface Settings
      - LDR_WEB__PORT=5000
      - LDR_WEB__HOST=0.0.0.0
    volumes:
      - ./deepresearch:/root/.config/local_deep_research
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest@sha256:0bae02d9fd3afe953263cb58a656142ed7828efb77e05ec9dd84223ca26b30db
    restart: unless-stopped
    labels:
      - traefik.enable=true
      - traefik.http.services.searxng.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    healthcheck:
      test: "wget --no-verbose --tries=1 --spider http://localhost:8080/ || exit 1"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
    # cap_drop:
    #   - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
networks:
  default:
    external: true
    name: ctr-network