services:
  ollama:
    image: ollama/ollama:0.13.0@sha256:d4188c1dfa870386a14e299976aed96daeb83876b69e1a852c9d09ea76463b9f
    ports:
      - 11434:11434
    labels:
      - traefik.enable=true
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./code:/code
      - /mnt/ssd512/ctr_data/ollama/ollama:/root/.ollama
    container_name: ollama
    tty: true
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  openwebui:
    image: ghcr.io/open-webui/open-webui:main@sha256:c0297cf2f76e0ba21d19eb3a21753d3b271c0247694cba3948677d2a97ec559f
    container_name: openwebui
    labels:
      - traefik.enable=true
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./openwebui:/app/backend/data
    depends_on:
      - ollama
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://ollama:11434 #comma separated ollama hosts
      - ENV=prod
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=REMOTE-EMAIL
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=REMOTE-NAME
    healthcheck:
      test: "curl -f http://localhost:8080/health"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
  deepresearch:
    image: localdeepresearch/local-deep-research:1.2.14@sha256:e34bed441bbc4f0d95dedddf82993d8cd8fb3fa577873054c21c6de4e08bd255
    restart: unless-stopped
    container_name: deepresearch
    labels:
      - traefik.enable=true
      - traefik.http.services.deepresearch.loadbalancer.server.port=5000
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # LLM Configuration
      - LDR_LLM__PROVIDER=ollama
      - LDR_LLM__MODEL=gemma3
      - LDR_LLM__TEMPERATURE=0.7
      
      # Search Configuration
      - LDR_SEARCH__TOOL=auto
      - LDR_SEARCH__ITERATIONS=2
      - LDR_SEARCH__QUESTIONS_PER_ITERATION=2

      # Searxng Configuration
      - SEARXNG_INSTANCE=http://searxng:8080
      - SEARXNG_DELAY=2.0
      
      # Web Interface Settings
      - LDR_WEB__PORT=5000
      - LDR_WEB__HOST=0.0.0.0
    volumes:
      - ./deepresearch:/root/.config/local_deep_research
  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest@sha256:fc076352d72154feb1d8c0eb42dd5570a3ebc9ca8c6b9c8318ce545a8dfd1ea4
    restart: unless-stopped
    labels:
      - traefik.enable=true
      - traefik.http.services.searxng.loadbalancer.server.port=8080
      - show.external=true
      - sablier.enable=true
      - sablier.group=ollama
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
    healthcheck:
      test: "wget --no-verbose --tries=1 --spider http://localhost:8080/ || exit 1"
      interval: 5m
      timeout: 15s
      retries: 5
      start_period: 30s
      start_interval: 5s
    # cap_drop:
    #   - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
networks:
  default:
    external: true
    name: ctr-network