services:
  ollama:
    image: ollama/ollama:0.6.5
    ports:
      - 11434:11434
    labels:
      - traefik.enable=true
      - traefik.http.services.ollama.loadbalancer.server.port=11434
      - show.external=true
    volumes:
      - ./code:/code
      - /mnt/ssd512/ctr_data/ollama/ollama:/root/.ollama
    container_name: ollama
    tty: true
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    labels:
      - traefik.enable=true
      - traefik.http.services.openwebui.loadbalancer.server.port=8080
      - show.external=true
    volumes:
      - ./openwebui:/app/backend/data
    depends_on:
      - ollama
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      - OLLAMA_BASE_URLS=http://ollama:11434 #comma separated ollama hosts
      - ENV=prod
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=REMOTE-EMAIL
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=REMOTE-NAME
  sdwebui:
    image: ghcr.io/neggles/sd-webui-docker:latest
    restart: unless-stopped
    container_name: sdwebui
    ports:
      - 7860:7860
    labels:
      - traefik.enable=true
      - traefik.http.services.sdwebui.loadbalancer.server.port=7860
      - show.external=true
    environment:
      CLI_ARGS: "--skip-version-check --allow-code --api --xformers --medvram --no-half"
      # make TQDM behave a little better
      PYTHONUNBUFFERED: "1"
      SD_WEBUI_VARIANT: auto
    volumes:
      - /mnt/ssd512/ctr_data/sdwebui/data:/data
      - ./output:/output
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
networks:
  default:
    external: true
    name: ctr-network